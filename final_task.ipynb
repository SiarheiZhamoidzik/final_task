{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8883a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow\n",
    "import numpy as np\n",
    "from textwrap import dedent\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30e21010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert files to pandas dataframe\n",
    "def to_pandas_df_converter(path: str, datatype: str, keep_default_na=True):\n",
    "    \"\"\"\n",
    "    Transforms data from csv or parquet files into pandas dataframe\n",
    "    :param path: path to file for processing. In case of parquet files it could be folder with several parquet files inside\n",
    "    :param datatype: definition of data type. Could be csv or parquet only\n",
    "    :param keep_default_na: optional parameter to handle NaN values\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if datatype == 'csv':\n",
    "            return pd.read_csv(path, keep_default_na=keep_default_na)\n",
    "        elif datatype == 'parquet':\n",
    "            return pd.read_parquet(path)\n",
    "        else: \n",
    "            raise ValueError('Improper datatype is chosen. Choose csv or parquet')\n",
    "    except (ValueError, PermissionError) as e:\n",
    "        raise Exception(\"Path to file is not defined propery. Please, check if path is proper in accordance with chosen datatype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e86b010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "      <th>File path</th>\n",
       "      <th>DQ check</th>\n",
       "      <th>Column</th>\n",
       "      <th>Status</th>\n",
       "      <th>Bad Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flights</td>\n",
       "      <td>./Data/source/flights.csv</td>\n",
       "      <td>Completeness by empty/non empty fields</td>\n",
       "      <td>Year, Month, DayofMonth, DayOfWeek, DepTime, C...</td>\n",
       "      <td>Failed</td>\n",
       "      <td>There are 34 rows with epmty values in any of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Table                  File path                                DQ check  \\\n",
       "0  flights  ./Data/source/flights.csv  Completeness by empty/non empty fields   \n",
       "\n",
       "                                              Column  Status  \\\n",
       "0  Year, Month, DayofMonth, DayOfWeek, DepTime, C...  Failed   \n",
       "\n",
       "                                            Bad Data  \n",
       "0  There are 34 rows with epmty values in any of ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check empty_nonempty fields\n",
    "\n",
    "def check_for_empy_fields(path: str, datatype: str, columns_to_check: list=False):\n",
    "    \"\"\"\n",
    "    Checks if file contains empty values in defined columns.\n",
    "    NOTE: ('NA', 'nan' values etc. are not considered as empty. Only empty string like ' ' is defined as empty)\n",
    "    :param path: path to file for processing. In case of parquet files it could be folder with several parquet files inside\n",
    "    :param datatype: definition of data type. Could be csv or parquet only\n",
    "    :param columns_to_check: columns to check for absence of empty values. If False (default) then all columns are checked\n",
    "    :return: pandas dataframe row with results of check\n",
    "    \"\"\"\n",
    "\n",
    "    # get table name for logs\n",
    "    table_name = path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    dataframe = to_pandas_df_converter(path, datatype, keep_default_na=False)\n",
    "\n",
    "    # get names of all columns if specific columns are not defined\n",
    "    if columns_to_check==False:\n",
    "        columns_to_check = list(dataframe.columns.values)\n",
    "\n",
    "    # create string with columns names for logs\n",
    "    string_of_cols = \", \".join([\"\" + str(item) + \"\" for item in columns_to_check])\n",
    "\n",
    "    dataframe_to_check = dataframe[columns_to_check]\n",
    "    count_of_rows_with_nulls = dataframe_to_check[dataframe_to_check.applymap(lambda x: True if (str(x).strip()=='') else False).any(axis=1)].shape[0]\n",
    "\n",
    "    if count_of_rows_with_nulls == 0:\n",
    "        status ='Passed'\n",
    "        bad_data_desc = ''\n",
    "    else: \n",
    "        status ='Failed'\n",
    "        bad_data_desc = f\"There are {count_of_rows_with_nulls} rows with epmty values in any of the chosen columns\"\n",
    "\n",
    "    data = {'Table': [table_name], \n",
    "            'File path': [path], \n",
    "            'DQ check': ['Completeness by empty/non empty fields'], \n",
    "            'Column': [string_of_cols], \n",
    "            'Status': [status], \n",
    "            'Bad Data': [bad_data_desc]}\n",
    "\n",
    "    result_table = pd.DataFrame(data)\n",
    "\n",
    "    return result_table\n",
    "\n",
    "\n",
    "# example of using\n",
    "path = './Data/source/flights.csv'\n",
    "path_parquet = './Data/raw/flights'\n",
    "\n",
    "list_of_columns = \\\n",
    "['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'CRSDepTime',\n",
    "'ArrTime', 'CRSArrTime', 'UniqueCarrier', 'FlightNum', 'TailNum',\n",
    "'ActualElapsedTime', 'CRSElapsedTime', 'AirTime', 'ArrDelay',\n",
    "'DepDelay', 'Origin', 'Dest', 'Distance', 'TaxiIn', 'TaxiOut', \n",
    "'Cancelled', 'Diverted', 'CarrierDelay', 'WeatherDelay',\n",
    "'NASDelay', 'SecurityDelay', 'LateAircraftDelay']\n",
    "\n",
    "nn_result = check_for_empy_fields(path, 'csv', list_of_columns)\n",
    "nn_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "783dae8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "      <th>File path</th>\n",
       "      <th>DQ check</th>\n",
       "      <th>Column</th>\n",
       "      <th>Status</th>\n",
       "      <th>Bad Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flights</td>\n",
       "      <td>./Data/source/flights.csv</td>\n",
       "      <td>Validity by range: min value: 0, max value: 2359</td>\n",
       "      <td>DepTime, CRSDepTime</td>\n",
       "      <td>Failed</td>\n",
       "      <td>There are 1 out of range values. \\nList of up ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Table                  File path  \\\n",
       "0  flights  ./Data/source/flights.csv   \n",
       "\n",
       "                                           DQ check               Column  \\\n",
       "0  Validity by range: min value: 0, max value: 2359  DepTime, CRSDepTime   \n",
       "\n",
       "   Status                                           Bad Data  \n",
       "0  Failed  There are 1 out of range values. \\nList of up ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check validity by range\n",
    "\n",
    "def check_validity_by_range(path: str, datatype: str, columns_to_check: list, min_val: int, max_val: int):\n",
    "    \"\"\"\n",
    "    Checks if in the specific columns there are values out of the [min, max] range.\n",
    "    NOTE: NaN values are not considered to be out of range.\n",
    "    :param path: path to the file for processing. In case of parquet files it could be folder with several parquet files inside\n",
    "    :param datatype: definition of data type. Could be csv or parquet only\n",
    "    :param columns_to_check: columns to check\n",
    "    :param min_val: min value in range\n",
    "    :param max_val: max value in range\n",
    "    :return: pandas dataframe row with results of check\n",
    "    \"\"\"\n",
    "    # get table name for logs\n",
    "    table_name = path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    dataframe = to_pandas_df_converter(path, datatype)\n",
    "\n",
    "    # create string with columns names for logs\n",
    "    string_of_cols = \", \".join([\"\" + str(item) + \"\" for item in columns_to_check])\n",
    "\n",
    "    out_of_range_values = []\n",
    "    for value in columns_to_check:\n",
    "        values_to_check = dataframe[value].tolist()\n",
    "        out_of_range_values_per_column = [val for val in values_to_check if (val<min_val or val>max_val) and (pd.isnull(val)==False or val!=np.nan)]\n",
    "        out_of_range_values+=out_of_range_values_per_column\n",
    "    \n",
    "    if len(out_of_range_values) == 0:\n",
    "        status ='Passed'\n",
    "        bad_data_desc = ''\n",
    "    else: \n",
    "        status ='Failed'\n",
    "        bad_data_desc = f\"There are {len(out_of_range_values)} out of range values. \\nList of up to 20 distinct out of range values: {set(out_of_range_values[0:20])}\"\n",
    "\n",
    "    data = {'Table': [table_name], \n",
    "            'File path': [path], \n",
    "            'DQ check': [f'Validity by range: min value: {min_val}, max value: {max_val}'], \n",
    "            'Column': [string_of_cols], \n",
    "            'Status': [status], \n",
    "            'Bad Data': [bad_data_desc]}\n",
    "    result_table = pd.DataFrame(data)\n",
    "\n",
    "    return result_table\n",
    "\n",
    "\n",
    "# example of using\n",
    "path = './Data/source/flights.csv'\n",
    "path_parquet = './Data/raw/flights'\n",
    "\n",
    "range_result = check_validity_by_range(path, 'csv', ['DepTime', 'CRSDepTime'], 0, 2359)\n",
    "range_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae38fc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "      <th>File path</th>\n",
       "      <th>DQ check</th>\n",
       "      <th>Column</th>\n",
       "      <th>Status</th>\n",
       "      <th>Bad Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>airports</td>\n",
       "      <td>./Data/source/airports.csv</td>\n",
       "      <td>Uniqueness by set of columns</td>\n",
       "      <td>iata</td>\n",
       "      <td>Failed</td>\n",
       "      <td>There are 6 duplicates. List of up to 10 dupli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Table                   File path                      DQ check Column  \\\n",
       "0  airports  ./Data/source/airports.csv  Uniqueness by set of columns   iata   \n",
       "\n",
       "   Status                                           Bad Data  \n",
       "0  Failed  There are 6 duplicates. List of up to 10 dupli...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check uniqueness\n",
    "\n",
    "def check_uniqueness(path: str, datatype: str, columns_to_check: list=False):\n",
    "    \"\"\"\n",
    "    Checks if file contains duplicates by defined columns\n",
    "    :param path: path to file for processing. In case of parquet files it could be folder with several parquet files inside\n",
    "    :param datatype: definition of data type. Could be csv or parquet only\n",
    "    :param columns_to_check: set of columns, which should unequally define values. If False (default) then all columns are checked\n",
    "    :return: pandas dataframe row with results of check\n",
    "    \"\"\"\n",
    "    # get table name for logs\n",
    "    table_name = path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    dataframe = to_pandas_df_converter(path, datatype)\n",
    "\n",
    "    # get dataframe columns for checks\n",
    "    if columns_to_check==False:\n",
    "        columns_to_check = list(dataframe.columns.values)\n",
    "\n",
    "    # create string with columns names for logs\n",
    "    string_of_cols = \", \".join([\"\" + str(item) + \"\" for item in columns_to_check])\n",
    "\n",
    "    dataframe_to_check = dataframe[columns_to_check].dropna()\n",
    "    duplicates = dataframe_to_check[dataframe_to_check.duplicated()]\n",
    "\n",
    "    if duplicates.shape[0] == 0:\n",
    "        status = 'Passed'\n",
    "        bad_data_desc = ''\n",
    "    else: \n",
    "        status = 'Failed'\n",
    "        bad_data_desc = f\"There are {duplicates.shape[0]} duplicates. List of up to 10 duplicated values:\\n {duplicates.to_string(index=False, max_rows=10, col_space=10)}\"\n",
    "\n",
    "    data = {'Table': [table_name], \n",
    "            'File path': [path], \n",
    "            'DQ check': ['Uniqueness by set of columns'], \n",
    "            'Column': [string_of_cols], \n",
    "            'Status': [status], \n",
    "            'Bad Data': [bad_data_desc]}\n",
    "    result_table = pd.DataFrame(data)\n",
    "\n",
    "    return result_table\n",
    "\n",
    "\n",
    "# example of using\n",
    "path = './Data/source/airports.csv'\n",
    "path_parquet = './Data/raw/airports'\n",
    "\n",
    "unq_result = check_uniqueness(path, 'csv', ['iata'])\n",
    "unq_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "659ccd25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "      <th>File path</th>\n",
       "      <th>DQ check</th>\n",
       "      <th>Column</th>\n",
       "      <th>Status</th>\n",
       "      <th>Bad Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>First table: carriers. \\nSecond table: flights</td>\n",
       "      <td>First path: ./Data/source/carriers.csv. \\nSeco...</td>\n",
       "      <td>Consistency between 2 columns</td>\n",
       "      <td>Column from first table: Code. \\nColumn from s...</td>\n",
       "      <td>Failed</td>\n",
       "      <td>There are 2 inappropriate values.\\n        Exa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Table  \\\n",
       "0  First table: carriers. \\nSecond table: flights   \n",
       "\n",
       "                                           File path  \\\n",
       "0  First path: ./Data/source/carriers.csv. \\nSeco...   \n",
       "\n",
       "                        DQ check  \\\n",
       "0  Consistency between 2 columns   \n",
       "\n",
       "                                              Column  Status  \\\n",
       "0  Column from first table: Code. \\nColumn from s...  Failed   \n",
       "\n",
       "                                            Bad Data  \n",
       "0  There are 2 inappropriate values.\\n        Exa...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consistency check. Compares unique values in 2 columns\n",
    "\n",
    "def check_consistency(first_path: str, first_datatype: str, first_column_to_check: str, second_path: str, second_datatype: str, second_column_to_check: str, comparison_type: str):\n",
    "    \"\"\"\n",
    "    Checks consistency between 2 columns\n",
    "    :param first_path: path to the first file for consistency check. In case of parquet files it could be folder with several parquet files inside\n",
    "    :param first_datatype: definition of data type for the first file. Could be csv or parquet only\n",
    "    :param first_column_to_check: columns from the first file for consistency check\n",
    "    :param second_path: path to the second file for consistency check. In case of parquet files it could be folder with several parquet files inside\n",
    "    :param second_datatype: definition of data type for the second file. Could be csv or parquet only\n",
    "    :param second_column_to_check: columns from the second file for consistency check\n",
    "    :param comparison_type: type of comparison. Appropriate values: \n",
    "                            'full_match' - unique values in both columns should be the same,\n",
    "                            'first_match_second' - unique values in the first table should exists in the second table,\n",
    "                            'second_match_first' - unique values in the second table should exists in the first table\n",
    "    :return: pandas dataframe row with results of check\n",
    "    \"\"\"\n",
    "    first_dataframe = to_pandas_df_converter(first_path, first_datatype)\n",
    "    first_dataframe_column = first_dataframe[[first_column_to_check]].drop_duplicates().dropna()\n",
    "    first_table_name = first_path.split('/')[-1].split('.')[0]\n",
    "\n",
    "    second_dataframe = to_pandas_df_converter(second_path, second_datatype)\n",
    "    second_dataframe_column = second_dataframe[[second_column_to_check]].drop_duplicates().dropna()\n",
    "    second_table_name = second_path.split('/')[-1].split('.')[0]\n",
    "\n",
    "    result_join = first_dataframe_column.merge(second_dataframe_column, left_on=[first_column_to_check], right_on=[second_column_to_check], how='outer', suffixes=('_ss', '_sap'), indicator=True)\n",
    "    \n",
    "    if comparison_type == 'full_match':\n",
    "        comparison_result = result_join[result_join._merge!='both']\n",
    "        bad_data_desc = \\\n",
    "        dedent(f'''There are {comparison_result.shape[0]} inappropriate values. {comparison_result[comparison_result['_merge']=='left_only'].shape[0]} in first table. {comparison_result[comparison_result['_merge']=='right_only'].shape[0]} in second table.\n",
    "        Example of up to 20 values, which exist in first table only: {comparison_result[comparison_result['_merge']=='left_only'][first_column_to_check].tolist()[:20]}.\n",
    "        Example of up to 20 values, which exist in second table only: {comparison_result[comparison_result['_merge']=='right_only'][second_column_to_check].tolist()[:20]}.''')\n",
    "    elif comparison_type == 'first_match_second':\n",
    "        comparison_result = result_join[result_join._merge=='left_only']\n",
    "        bad_data_desc = \\\n",
    "        f'''There are {comparison_result.shape[0]} inappropriate values.\\\n",
    "        Example of up to 20 values, which exist in first table only: {comparison_result[comparison_result['_merge']=='left_only'][first_column_to_check].tolist()[:20]}.'''\n",
    "    elif comparison_type == 'second_match_first':\n",
    "        comparison_result = result_join[result_join._merge=='right_only']\n",
    "        bad_data_desc = \\\n",
    "        dedent(f'''There are {comparison_result.shape[0]} inappropriate values.\n",
    "        Example of up to 20 values, which exist in second table only: {comparison_result[comparison_result['_merge']=='right_only'][second_column_to_check].tolist()[:20]}.''').strip(\"\\n\")\n",
    "    else:\n",
    "        raise Exception(\"Comparison type is not defined properly. Should be 'full_match' or first_match_second' or 'second_match_first' only.\")\n",
    "\n",
    "    if comparison_result.shape[0] == 0:\n",
    "        status ='Passed'\n",
    "        bad_data_desc = ''\n",
    "    else: \n",
    "        status ='Failed'\n",
    "\n",
    "    data = {'Table': [f'''First table: {first_table_name}. \\nSecond table: {second_table_name}'''], \n",
    "            'File path': [f\"First path: {first_path}. \\nSecond path: {second_path}\"], \n",
    "            'DQ check': [f'Consistency between 2 columns'], \n",
    "            'Column': [f\"Column from first table: {first_column_to_check}. \\nColumn from second table: {second_column_to_check}\"], \n",
    "            'Status': [status], \n",
    "            'Bad Data': [bad_data_desc]}\n",
    "    result_table = pd.DataFrame(data)   \n",
    "\n",
    "    return result_table\n",
    "\n",
    "\n",
    "# example of using\n",
    "first_path = './Data/source/carriers.csv'\n",
    "second_path = './Data/source/flights.csv'\n",
    "\n",
    "cons_result = check_consistency(first_path=first_path, first_datatype='csv', first_column_to_check='Code', second_path=second_path, second_datatype='csv', second_column_to_check='UniqueCarrier', comparison_type='second_match_first')\n",
    "cons_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6530f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completeness check. Full join comparison \n",
    "\n",
    "def check_pk_completeness(first_path: str, first_datatype: str, second_path: str, second_datatype: str, first_join_keys: list=False, second_join_keys: list=False):\n",
    "    \"\"\"\n",
    "    Check if all Primary keys exists in both tables\n",
    "    :param first_path: path to the first file. In case of parquet files it could be folder with several parquet files inside\n",
    "    :param first_datatype: definition of data type for the first file. Could be csv or parquet only\n",
    "    :param first_join_keys: columns from the first file for joining. If False - all columns\n",
    "    :param second_path: path to the second file for. In case of parquet files it could be folder with several parquet files inside\n",
    "    :param second_datatype: definition of data type for the second file. Could be csv or parquet only\n",
    "    :param second_join_keys: columns from the second file for joining. If False - all columns\n",
    "    :return: pandas dataframe row with results of check\n",
    "    \"\"\"  \n",
    "    first_dataframe = to_pandas_df_converter(first_path, first_datatype)\n",
    "    first_dataframe = first_dataframe.astype(str)\n",
    "    first_table_name = first_path.split('/')[-1].split('.')[0]    \n",
    "    # get all columns\n",
    "    if first_join_keys==False:\n",
    "        first_join_keys = list(first_dataframe.columns.values)\n",
    "\n",
    "    second_dataframe = to_pandas_df_converter(second_path, second_datatype)\n",
    "    second_dataframe = second_dataframe.astype(str)\n",
    "    second_table_name = second_path.split('/')[-1].split('.')[0] \n",
    "    # get all columns\n",
    "    if second_join_keys==False:\n",
    "        second_join_keys = list(second_dataframe.columns.values)\n",
    "\n",
    "    result_join = first_dataframe.merge(second_dataframe, left_on=first_join_keys, right_on=second_join_keys, how='outer', suffixes=('_ss', '_sap'), indicator=True)\n",
    "\n",
    "    check_result = result_join.query(\"_merge != 'both'\")\n",
    "\n",
    "    if check_result.shape[0] == 0:\n",
    "        status ='Passed'\n",
    "    else: \n",
    "        status ='Failed'\n",
    "        bad_data_desc = f'''There are {check_result.query(\"_merge == 'left_only'\").shape[0]} rows which exist in the first table, while are absent in the second one. \n",
    "There are {check_result.query(\"_merge == 'right_only'\").shape[0]} rows which exist in the second table, while are absent in the first one.\n",
    "Up to 10 rows with difference:\n",
    "        {check_result.to_string(index=False, max_rows=10, col_space=10)}'''\n",
    "\n",
    "    data = {'Table': [f'''First table: {first_table_name}. \\nSecond table: {second_table_name}'''], \n",
    "            'File path': [f\"First path: {first_path}. \\nSecond path: {second_path}\"], \n",
    "            'DQ check': [f'Consistency between 2 columns'], \n",
    "            'Column': [f\"Join keys from the first table: {first_join_keys}. \\nJoin keys from the second table: {second_join_keys}\"], \n",
    "            'Status': [status], \n",
    "            'Bad Data': [bad_data_desc]}\n",
    "    result_table = pd.DataFrame(data)   \n",
    "\n",
    "    return result_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64a3cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "# create an empty dataframe for result table\n",
    "result_table = pd.DataFrame(columns = ['Table', 'File path', 'DQ check', 'Column', 'Status', 'Bad Data'])\n",
    "\n",
    "\n",
    "# Carriers checks\n",
    "unq_result = check_uniqueness('./Data/source/carriers.csv', 'csv', False)\n",
    "result_table = pd.concat([result_table, unq_result], ignore_index=True)\n",
    "\n",
    "nn_result = check_for_empy_fields('./Data/source/carriers.csv', 'csv', False)\n",
    "result_table = pd.concat([result_table, nn_result], ignore_index=True)\n",
    "\n",
    "comp_result = check_pk_completeness(first_path='./Data/source/carriers.csv', first_datatype='csv', first_join_keys=['Code', 'Description'], \\\n",
    "                                    second_path='./Data/raw/carriers', second_datatype='parquet', second_join_keys=['code', 'description'])\n",
    "\n",
    "\n",
    "# Airports checks\n",
    "unq_result = check_uniqueness('./Data/source/airports.csv', 'csv', ['iata'])\n",
    "result_table = pd.concat([result_table, unq_result], ignore_index=True)\n",
    "\n",
    "nn_result = check_for_empy_fields('./Data/source/airports.csv', 'csv', False)\n",
    "result_table = pd.concat([result_table, nn_result], ignore_index=True)\n",
    "\n",
    "range_result = check_validity_by_range('./Data/source/airports.csv', 'csv', ['lat'], -90, 90)\n",
    "result_table = pd.concat([result_table, range_result], ignore_index=True)\n",
    "\n",
    "range_result = check_validity_by_range('./Data/source/airports.csv', 'csv', ['long'], -180, 180)\n",
    "result_table = pd.concat([result_table, range_result], ignore_index=True)\n",
    "\n",
    "comp_result = check_pk_completeness(first_path='./Data/source/airports.csv', first_datatype='csv', first_join_keys=['iata'], \\\n",
    "                                    second_path='./Data/raw/airports', second_datatype='parquet', second_join_keys=['iata'])\n",
    "\n",
    "\n",
    "# Flights checks\n",
    "unq_result = check_uniqueness('./Data/source/flights.csv', 'csv', ['Year', 'Month', 'DayofMonth', 'DepTime', 'FlightNum'])\n",
    "result_table = pd.concat([result_table, unq_result], ignore_index=True)\n",
    "\n",
    "list_of_not_empty_columns = \\\n",
    "['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'CRSDepTime',\n",
    "'ArrTime', 'CRSArrTime', 'UniqueCarrier', 'FlightNum', 'TailNum',\n",
    "'ActualElapsedTime', 'CRSElapsedTime', 'AirTime', 'ArrDelay',\n",
    "'DepDelay', 'Origin', 'Dest', 'Distance', 'TaxiIn', 'TaxiOut', \n",
    "'Cancelled', 'Diverted', 'CarrierDelay', 'WeatherDelay',\n",
    "'NASDelay', 'SecurityDelay', 'LateAircraftDelay']\n",
    "nn_result = check_for_empy_fields('./Data/source/flights.csv', 'csv', list_of_not_empty_columns)\n",
    "result_table = pd.concat([result_table, nn_result], ignore_index=True)\n",
    "\n",
    "range_result = check_validity_by_range('./Data/source/flights.csv', 'csv', ['DepTime', 'CRSDepTime'], 0, 2359)\n",
    "result_table = pd.concat([result_table, range_result], ignore_index=True)\n",
    "\n",
    "range_result = check_validity_by_range('./Data/source/flights.csv', 'csv', ['ArrTime', 'CRSArrTime'], 0, 2359)\n",
    "result_table = pd.concat([result_table, range_result], ignore_index=True)\n",
    "\n",
    "cons_result = check_consistency(first_path='./Data/source/carriers.csv', \n",
    "                                first_datatype='csv', \n",
    "                                first_column_to_check='Code', \n",
    "                                second_path='./Data/source/flights.csv', \n",
    "                                second_datatype='csv', \n",
    "                                second_column_to_check='UniqueCarrier', \n",
    "                                comparison_type='second_match_first')\n",
    "result_table = pd.concat([result_table, cons_result], ignore_index=True)\n",
    "\n",
    "cons_result = check_consistency(first_path='./Data/source/airports.csv', \n",
    "                                first_datatype='csv', \n",
    "                                first_column_to_check='iata', \n",
    "                                second_path='./Data/source/flights.csv', \n",
    "                                second_datatype='csv', \n",
    "                                second_column_to_check='Origin', \n",
    "                                comparison_type='second_match_first')\n",
    "result_table = pd.concat([result_table, cons_result], ignore_index=True)\n",
    "\n",
    "cons_result = check_consistency(first_path='./Data/source/airports.csv', \n",
    "                                first_datatype='csv', \n",
    "                                first_column_to_check='iata', \n",
    "                                second_path='./Data/source/flights.csv', \n",
    "                                second_datatype='csv', \n",
    "                                second_column_to_check='Dest', \n",
    "                                comparison_type='second_match_first')\n",
    "result_table = pd.concat([result_table, cons_result], ignore_index=True)\n",
    "\n",
    "comp_result = check_pk_completeness(first_path='./Data/source/flights.csv', first_datatype='csv', first_join_keys=['Year', 'Month', 'DayofMonth', 'FlightNum'], \\\n",
    "                                    second_path='./Data/raw/flights', second_datatype='parquet', second_join_keys=['Year', 'Month', 'DayofMonth', 'FlightNum'])\n",
    "\n",
    "# write results to excel file\n",
    "result_path=f'./Results/total_result_{now}.xlsx'.replace(':','-')\n",
    "result_table.to_excel(excel_writer=result_path, sheet_name='results')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8191881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local PySpark (Python-3.7.9 / Spark-3.0.1 )",
   "language": "python",
   "name": "py3spark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
